---
title: A 15-Minute Experiment
date: 2026-01-22
---

The group‚Äôs first chance to roll up their sleeves came to join small groups and generate an exhaustive list of research components that could be reviewed, evaluated, or given feedback on as an independent component.

Participants were explicitly asked to work quickly. The exercise was messy by design, embracing incompleteness to surface ideas quickly, with the understanding that the next six months would allow the group to return, iterate, and reshape what emerged.

There was no debate. No prioritization. No pressure to be right. No wrong answers.

To spark thinking, the group drew inspiration from a case study outside academia: [building reviews and inspections](https://articles.continuousfoundation.org/articles/building-code-review) conducted by specialists at different stages, rather than a single approval at the end. That shift offered a useful parallel for imagining how peer review might operate across review purposes and research components over time, instead of arriving only once everything is bundled together.

## Reviewable, Modular Components

**In 15 minutes, the paper shrank from destination to a singular waypoint.** Research questions and study design. Ethics approvals and position statements. Data, materials, and metadata. Code, models, and computational notebooks. Analysis, interpretation, figures, manuscripts. Communication artifacts. Reviews themselves.

:::{figure #fig-iceberg} #iceberg
The deep work behind papers and manuscripts, all of which can be reviewed.
:::

**Research Design & Framing**

- Research question
- Conceptualization
- Rationale for the approach
- Study design
- Study protocol
- Study plan / proposal
- Preregistration / pre-registration
- Hypotheses / hypothesis building
- Sampling plan
- Sample composition
- Study power
- Statistical analysis plan
- Plans for materials (MDAR)
- Methodological review board proposals

**Ethics, Integrity & Governance**

- IRB applications
- Ethical and permissions documentation
- Data protection plans (anonymization, consent)
- Position statements
- Trust signals
- Research integrity checks
- Review of reviews
- Ethics approval / ethics statement

**Data & Material**

- Data collection plan
- Data collection instruments
- Raw data
- Final datasets
- Data availability
- Data FAIRness
- Data management plan
- Data retention plan
- Genomic sequences / genomes
- Protein structures
- Reagents
- Materials availability

**Code, Software & Computation**

- Analysis code
- Data cleaning / preparation code
- Statistical code
- Scripts
- Computational notebooks
- Research software
- Software packages / libraries used
- Models (including machine learning models)
- Image (pre)processing workflows
- AI prompts / chat transcripts
- Code, search details, and AI prompts

**Analysis & Interpretation**

- Analysis
- Data analysis
- Statistical analysis
- Interpretation
- Analytic interpretation
- Main claims
- Limitations of the research
- Implications / impact
- Practical implications

**Outputs & Research Objects**

- Figures
- Tables
- Data visualizations
- Figures and tables together
- Manuscript / Paper
- Preprint
- Discussion section
- Results datasets

**Communication & Knowledge Translation**

- Lay summaries
- Language and framing
- Accessibility (colors, screen readers, etc.)
- Policy briefs
- Posters
- Talks / presentations
- Introductory talks (with or without data)
- Videos
- Video methods
- Related outputs (posters, talks, media)

**Metadata, Attribution & Infrastructure**

- Metadata
- Study metadata (for archiving)
- Completeness of metadata
- Citations / references
- Reporting guidelines
- Availability of different components
- Openness statements
- Author contributions
- Authorship / creator attribution
- Acknowledgements

**Review & Evaluation Artifacts**

- Peer review reports
- Post-publication comments
- Reviews themselves as objects

**Funding & Planning**

- Grant propositions (pre-study)
- Funding plans / proposals
- Grants

**Emerging Artifacts**

- Performative arts-sourced research artifacts
- Time-based aspects of review

The lists above shows an imperfect but initial categorization of that list, created only to make its breadth visible. These categories are intentionally loose, not a taxonomy or a roadmap, but a snapshot of collective thinking, captured before it hardens.

:::{card} üìã Full List of Components
:url: https://docs.google.com/spreadsheets/d/1ipMU6eMNzz22BeswmD6nUKdxtP9yCWK5_ao5Hd4ph1Q/edit?gid=0#gid=0
A full list of the components that we brainstormed together.
:::

### Why Share This Now

This list is not _done_. It is meant to _exist_ and be _improved_. It reflects what a diverse group surfaced together and think beyond inherited containers. Some components overlap. Some feel obvious. Others feel uncomfortable or under-explored. That unevenness is a signal that there is real work to do.

By sharing this early, the group is inviting others into the process.

## Summary of Ideation Reflection

The group also shared some existing research on openness promotion guidelines [@10.31222/osf.io/nmfs6_v2] and discussed the links between peer-review and editorial checks and policies (e.g. [F1000](https://f1000research.com/about/policies)).

- Strong alignment on the need for modular review:
  - The group broadly agreed that reviewing entire papers obscures important contributions, and that modular review better reflects how research is actually produced and used.
- Clear appetite for specialization in review:
  - Many noted that different components require different expertise, for example, statisticians, data editors, clinicians, software reviewers, rhetoricians, and that modular review could allow people to contribute where they are strongest.
- Disciplinary diversity matters
  - Multiple reflections emphasized that what is ‚Äúniche‚Äù in one field may be essential in another, and that modular systems must accommodate field-specific norms rather than impose a single model.
- The purpose of the review needs clarification.
  - Some participants flagged discomfort with moving forward without clearly defining what ‚Äúreview‚Äù is for, whether feedback, validation, decision-making, or post-publication evaluation, highlighting this as an important area for future work.
- Existing practices can be scaled:
  - Several noted that many forms of modular or informal review already happen today, including sleuthing, micro-review, and social media critique, but are undervalued or invisible in formal systems.
- Opportunities beyond traditional academics:
  - Some highlighted the potential for modular review to engage non-scholars, practitioners, and contributors who are often excluded from conventional peer review.
- Time and process trade-offs were visible:
  - Some groups found the spreadsheet exercise productive for capture but limiting for discussion, surfacing a tension between documentation and dialogue that the group may want to design around going forward.
- Overall tone was energized and collaborative:
  - Reflections consistently described excitement, ‚Äúlow-hanging fruit,‚Äù and a sense that this work is both timely and achievable, paired with appreciation for the open, exploratory approach to tackling it.
